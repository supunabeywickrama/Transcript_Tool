{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlUhAtoP/4T1q3WpU++TFh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supunabeywickrama/Transcript_Tool/blob/main/transcript_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W0fjuHN-J-Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4168b6d-454e-4833-a510-477ab4132d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# â¬‡ï¸ System + Python deps\n",
        "!apt -y install ffmpeg\n",
        "!pip -q install faster-whisper==1.0.3 yt-dlp youtube-transcript-api==0.6.2 srt==3.5.3 webvtt-py==0.5.1 python-dotenv==1.0.1\n",
        "# Optional cloud backend:\n",
        "!pip -q install openai==1.52.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âš™ï¸ Configure where outputs will be saved\n",
        "USE_DRIVE = True   # â† set False to keep outputs in /content/outputs\n",
        "\n",
        "SAVE_DIR = \"/content/outputs\"\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    SAVE_DIR = \"/content/drive/MyDrive/transcripts\"\n",
        "\n",
        "import os\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "SAVE_DIR\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DeglUXN5LiRI",
        "outputId": "5cc70b99-5fa3-42d8-d6f1-642e45236423"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/transcripts'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§¾ Helpers for JSON, SRT, VTT\n",
        "import json\n",
        "from typing import List, Dict\n",
        "\n",
        "def _sec_to_timestamp(sec: float, srt: bool = True) -> str:\n",
        "    h = int(sec // 3600); m = int((sec % 3600) // 60); s = sec % 60\n",
        "    return (f\"{h:02d}:{m:02d}:{s:06.3f}\".replace(\".\", \",\")\n",
        "            if srt else f\"{h:02d}:{m:02d}:{s:06.3f}\".replace(\",\", \".\"))\n",
        "\n",
        "def to_srt(segments: List[Dict]) -> str:\n",
        "    lines = []\n",
        "    for i, seg in enumerate(segments, 1):\n",
        "        start = _sec_to_timestamp(seg[\"start\"], srt=True)\n",
        "        end   = _sec_to_timestamp(seg[\"end\"],   srt=True)\n",
        "        text  = seg[\"text\"].strip()\n",
        "        lines += [str(i), f\"{start} --> {end}\", text, \"\"]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def to_vtt(segments: List[Dict]) -> str:\n",
        "    lines = [\"WEBVTT\", \"\"]\n",
        "    for seg in segments:\n",
        "        start = _sec_to_timestamp(seg[\"start\"], srt=False)\n",
        "        end   = _sec_to_timestamp(seg[\"end\"],   srt=False)\n",
        "        text  = seg[\"text\"].strip()\n",
        "        lines += [f\"{start} --> {end}\", text, \"\"]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def persist_outputs(basepath: str, result: Dict) -> Dict[str, str]:\n",
        "    json_path = basepath + \".json\"\n",
        "    srt_path  = basepath + \".srt\"\n",
        "    vtt_path  = basepath + \".vtt\"\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "    with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(to_srt(result[\"segments\"]))\n",
        "    with open(vtt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(to_vtt(result[\"segments\"]))\n",
        "    return {\"json\": json_path, \"srt\": srt_path, \"vtt\": vtt_path}\n"
      ],
      "metadata": {
        "id": "w9rPTy75MAJi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§  Local (offline) transcription with faster-whisper\n",
        "from typing import Optional\n",
        "\n",
        "def transcribe_local(audio_path: str, language: Optional[str] = None, model_size: str = \"small\") -> dict:\n",
        "    from faster_whisper import WhisperModel\n",
        "    model = WhisperModel(model_size, device=\"auto\", compute_type=\"auto\")\n",
        "    segments_iter, info = model.transcribe(audio_path, language=language, vad_filter=True, beam_size=5)\n",
        "\n",
        "    segments, full = [], []\n",
        "    for seg in segments_iter:\n",
        "        segments.append({\"start\": float(seg.start), \"end\": float(seg.end), \"text\": seg.text})\n",
        "        full.append(seg.text)\n",
        "\n",
        "    return {\n",
        "        \"backend\":  \"local\",\n",
        "        \"model\":    model_size,\n",
        "        \"language\": info.language,\n",
        "        \"duration\": sum(max(0.0, s[\"end\"] - s[\"start\"]) for s in segments),\n",
        "        \"segments\": segments,\n",
        "        \"text\":     \" \".join(full).strip()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Uc8zKb_xMCVq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â˜ï¸ Cloud transcription with OpenAI Whisper (optional)\n",
        "# Set your key in a cell:   import os; os.environ[\"OPENAI_API_KEY\"]=\"sk-...\"\n",
        "from typing import Optional\n",
        "\n",
        "def transcribe_openai(audio_path: str, language: Optional[str] = None) -> dict:\n",
        "    import os\n",
        "    from openai import OpenAI\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        raise RuntimeError(\"Set OPENAI_API_KEY env var to use OpenAI backend.\")\n",
        "    client = OpenAI()\n",
        "    with open(audio_path, \"rb\") as f:\n",
        "        resp = client.audio.transcriptions.create(model=\"whisper-1\", file=f, language=language)\n",
        "    text = resp.text.strip()\n",
        "    return {\n",
        "        \"backend\":  \"openai\",\n",
        "        \"model\":    \"whisper-1\",\n",
        "        \"language\": language or \"\",\n",
        "        \"duration\": 0.0,\n",
        "        \"segments\": [{\"start\": 0.0, \"end\": 0.0, \"text\": text}],\n",
        "        \"text\":     text\n",
        "    }\n"
      ],
      "metadata": {
        "id": "R3_uSBbRMEbC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â–¶ï¸ YouTube: try official captions first; otherwise download audio & transcribe\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
        "import yt_dlp, os\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "def try_fetch_youtube_captions(url: str, preferred_langs=(\"en\", \"en-US\", \"auto\")) -> Optional[List[Dict]]:\n",
        "    try:\n",
        "        # yt_dlp resolves the ID robustly\n",
        "        ydl_opts = {\"quiet\": True, \"skip_download\": True}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=False)\n",
        "            video_id = info.get(\"id\")\n",
        "        if not video_id:\n",
        "            return None\n",
        "\n",
        "        for lang in preferred_langs:\n",
        "            try:\n",
        "                return YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        transcripts = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "        for tr in transcripts:\n",
        "            try:\n",
        "                return tr.fetch()\n",
        "            except Exception:\n",
        "                pass\n",
        "    except (TranscriptsDisabled, NoTranscriptFound, Exception):\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def download_youtube_audio(url: str, outdir: str) -> str:\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    ydl_opts = {\n",
        "        \"format\": \"bestaudio/best\",\n",
        "        \"outtmpl\": os.path.join(outdir, \"%(id)s.%(ext)s\"),\n",
        "        \"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"mp3\", \"preferredquality\": \"192\"}],\n",
        "        \"quiet\": True,\n",
        "        \"nocheckcertificate\": True,\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=True)\n",
        "        return os.path.join(outdir, f\"{info['id']}.mp3\")\n"
      ],
      "metadata": {
        "id": "nMyLaDeFMGwz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§© High-level wrappers to produce JSON/SRT/VTT into SAVE_DIR\n",
        "from datetime import datetime\n",
        "import tempfile, shutil, os\n",
        "\n",
        "def transcribe_file_to_dir(in_path: str, save_dir: str, backend: str = \"local\",\n",
        "                           language: Optional[str] = None, model_size: str = \"small\") -> dict:\n",
        "    if backend == \"openai\":\n",
        "        result = transcribe_openai(in_path, language)\n",
        "    else:\n",
        "        result = transcribe_local(in_path, language, model_size=model_size)\n",
        "\n",
        "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    base  = os.path.join(save_dir, f\"transcript_{stamp}\")\n",
        "    paths = persist_outputs(base, result)\n",
        "    return {\"paths\": paths, \"result\": result}\n",
        "\n",
        "def transcribe_youtube_to_dir(url: str, save_dir: str, backend: str = \"local\",\n",
        "                              language: Optional[str] = None, model_size: str = \"small\") -> dict:\n",
        "    caps = try_fetch_youtube_captions(url)\n",
        "    if caps:\n",
        "        segments = []\n",
        "        for c in caps:\n",
        "            start = float(c[\"start\"]); end = start + float(c[\"duration\"])\n",
        "            segments.append({\"start\": start, \"end\": end, \"text\": c[\"text\"]})\n",
        "        result = {\n",
        "            \"backend\":  \"youtube_captions\",\n",
        "            \"model\":    \"captions\",\n",
        "            \"language\": language or \"\",\n",
        "            \"duration\": sum(max(0.0, s[\"end\"] - s[\"start\"]) for s in segments),\n",
        "            \"segments\": segments,\n",
        "            \"text\":     \" \".join(s[\"text\"] for s in segments).strip()\n",
        "        }\n",
        "        stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        base  = os.path.join(save_dir, f\"transcript_{stamp}\")\n",
        "        paths = persist_outputs(base, result)\n",
        "        return {\"paths\": paths, \"result\": result}\n",
        "    else:\n",
        "        with tempfile.TemporaryDirectory() as tmp:\n",
        "            audio = download_youtube_audio(url, tmp)\n",
        "            return transcribe_file_to_dir(audio, save_dir, backend=backend, language=language, model_size=model_size)\n"
      ],
      "metadata": {
        "id": "RLMa3JFCMJb7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”— Paste a YouTube URL (with or without captions)\n",
        "youtube_url = \"https://www.youtube.com/watch?v=vYqP-3ijl-8\"  # e.g., \"https://www.youtube.com/watch?v=xxxxxxxxxxx\"\n",
        "\n",
        "backend    = \"local\"   # \"local\" (offline) or \"openai\" (cloud, usually no per-segment times)\n",
        "language   = None      # e.g., \"en\" / \"si\" / \"ta\"\n",
        "model_size = \"small\"   # tiny | base | small | medium | large-v3 (local only)\n",
        "\n",
        "# ---------- helpers for paragraph view ----------\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import os\n",
        "\n",
        "def _fmt_mmss(sec: float) -> str:\n",
        "    if sec < 0: sec = 0.0\n",
        "    m = int(sec // 60)\n",
        "    s = int(round(sec % 60))\n",
        "    return f\"{m:02d}:{s:02d}\"\n",
        "\n",
        "def _group_segments_into_paragraphs(\n",
        "    segments: List[Dict],\n",
        "    max_gap_s: float = 1.2,       # silence gap to start a new paragraph\n",
        "    max_chars: int = 380,         # soft cap on paragraph text length\n",
        "    max_duration_s: float = 30.0  # hard cap on paragraph time span\n",
        ") -> List[Tuple[float, float, str]]:\n",
        "    \"\"\"\n",
        "    Returns a list of (start_sec, end_sec, paragraph_text).\n",
        "    Works with segments from YouTube captions OR local transcription.\n",
        "    \"\"\"\n",
        "    out: List[Tuple[float, float, str]] = []\n",
        "    if not segments:\n",
        "        return out\n",
        "\n",
        "    cur_start = float(segments[0][\"start\"])\n",
        "    cur_end   = float(segments[0][\"end\"])\n",
        "    cur_texts = [segments[0][\"text\"].strip()]\n",
        "    cur_chars = len(cur_texts[0])\n",
        "    prev_end  = cur_end\n",
        "\n",
        "    for seg in segments[1:]:\n",
        "        s = float(seg[\"start\"]); e = float(seg[\"end\"]); t = seg[\"text\"].strip()\n",
        "        gap = max(0.0, s - prev_end)\n",
        "        next_chars = cur_chars + (1 if cur_texts else 0) + len(t)\n",
        "        next_duration = max(cur_end, e) - cur_start\n",
        "\n",
        "        if (gap > max_gap_s) or (next_chars > max_chars) or (next_duration > max_duration_s):\n",
        "            out.append((cur_start, cur_end, \" \".join(cur_texts).strip()))\n",
        "            cur_start, cur_end, cur_texts, cur_chars = s, e, [t], len(t)\n",
        "        else:\n",
        "            cur_end = max(cur_end, e)\n",
        "            cur_texts.append(t)\n",
        "            cur_chars = next_chars\n",
        "\n",
        "        prev_end = e\n",
        "\n",
        "    out.append((cur_start, cur_end, \" \".join(cur_texts).strip()))\n",
        "    return out\n",
        "\n",
        "def _save_paragraphs_txt(paragraphs: List[Tuple[float, float, str]], basepath_no_ext: str) -> str:\n",
        "    out_txt = basepath_no_ext + \".txt\"\n",
        "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "        for (st, en, txt) in paragraphs:\n",
        "            f.write(f\"{_fmt_mmss(st)}â€“{_fmt_mmss(en)} â€” {txt}\\n\\n\")\n",
        "    return out_txt\n",
        "\n",
        "# ---------- run ----------\n",
        "if youtube_url.strip():\n",
        "    out = transcribe_youtube_to_dir(\n",
        "        youtube_url.strip(), SAVE_DIR,\n",
        "        backend=backend, language=language, model_size=model_size\n",
        "    )\n",
        "    paths   = out[\"paths\"]\n",
        "    result  = out[\"result\"]\n",
        "    segs    = result.get(\"segments\", [])\n",
        "\n",
        "    # Build and print paragraphs\n",
        "    paragraphs = _group_segments_into_paragraphs(\n",
        "        segs, max_gap_s=1.2, max_chars=380, max_duration_s=30.0\n",
        "    )\n",
        "\n",
        "    print(\"========== FULL TRANSCRIPT (paragraphs with timestamps) ==========\\n\")\n",
        "    for (st, en, txt) in paragraphs:\n",
        "        print(f\"{_fmt_mmss(st)}â€“{_fmt_mmss(en)} â€” {txt}\\n\")\n",
        "\n",
        "    # Save a .txt next to JSON/SRT/VTT\n",
        "    base_no_ext = os.path.splitext(paths[\"json\"])[0]\n",
        "    txt_path = _save_paragraphs_txt(paragraphs, base_no_ext)\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"Saved files:\")\n",
        "    print(paths)             # JSON / SRT / VTT\n",
        "    print({\"txt\": txt_path}) # Paragraphs TXT\n",
        "else:\n",
        "    print(\"Set youtube_url and run this cell.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOG6h54SMvm3",
        "outputId": "bcbca429-700c-478c-b5f1-471228b3e8ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] Falling back to generic n function search\n",
            "         player = https://www.youtube.com/s/player/c6d7bdc9/player_ias.vflset/en_US/base.js\n",
            "WARNING: [youtube] vYqP-3ijl-8: nsig extraction failed: Some formats may be missing\n",
            "         n = DQm65h5UEtgv1pNV ; player = https://www.youtube.com/s/player/c6d7bdc9/player_ias.vflset/en_US/base.js\n",
            "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
            "WARNING: [youtube] vYqP-3ijl-8: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] vYqP-3ijl-8: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] Falling back to generic n function search\n",
            "         player = https://www.youtube.com/s/player/c6d7bdc9/player_ias.vflset/en_US/base.js\n",
            "WARNING: [youtube] vYqP-3ijl-8: nsig extraction failed: Some formats may be missing\n",
            "         n = _aZS1Tpp7qgeym9j ; player = https://www.youtube.com/s/player/c6d7bdc9/player_ias.vflset/en_US/base.js\n",
            "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
            "WARNING: [youtube] vYqP-3ijl-8: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] vYqP-3ijl-8: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== FULL TRANSCRIPT (paragraphs with timestamps) ==========\n",
            "\n",
            "00:00â€“00:19 â€” Pressure's mounting for Andrew Mountbatten-Windsor formally prints Andrew to give evidence in the United States about his former friend, the late convicted pedophile Jeffrey Epstein. While victims of Epstein, as well as several members of Congress, have called for the former prints to speak to the authorities to tell them exactly what he knew and when.\n",
            "\n",
            "00:19â€“00:37 â€” While Andrew, who's now been stripped of all royal titles, has always denied any personal wrongdoing, John Sudworth has the latest on that. Following his history-making announcement, photographers captured the first glimpse of King Charles driving on his Sandringham estate, while for now his younger brother\n",
            "\n",
            "00:37â€“01:01 â€” remains here at Windsor's Royal Lodge, the home he soon to lose, along with all his titles. But far from drawing a line, in the US it's led to renewed calls for Andrew to reveal all he knows about his friend and convicted pedophile, the late Jeffrey Epstein. And to make good on the palace claim that its sympathies lie with survivors of abuse.\n",
            "\n",
            "01:01â€“01:28 â€” Words without action, really meaningless. So I would want to see some action on his part to back up what he said. He contends that he had no involvement. And so I think that a lot of us are curious as to why he's unwilling to cooperate and be questioned about his involvement with Epstein. If he has nothing to hide, then why is he hiding?\n",
            "\n",
            "01:28â€“01:50 â€” Newly unveiled US court documents once again show the depth of the friendship. In an email sent in 2010, a few months after Epstein was released from his first prison stint, Andrew writes he'll try to visit him in New York for a couple of days before the summer, adding it'd be good to catch up in person. It's all leading to mounting political pressure, too.\n",
            "\n",
            "01:51â€“02:13 â€” Members of the Congressional Committee investigating the handling of the Epstein case have been calling for Andrew to testify and threatening to compel him if he were ever to revisit the US. Come clean. Come before the US Congress. Voluntarily testify. Don't wait for a subpoena. Come and testify and tell us what you know.\n",
            "\n",
            "02:13â€“02:36 â€” At the end of the day, we want to know exactly what happened. Not just to give justice to the survivors, but to prevent this from ever happening again. Even as Andrew's royal status weakens, the scrutiny may yet intensify. Jon Sudworth, BBC News. Sarah Gristwood is an author and historian, and I spoke to her earlier\n",
            "\n",
            "02:36â€“03:01 â€” and asked her when she thinks Andrew will make that move to Sandringham. It's looking like after Christmas, which is no doubt a simply it takes a long time to move out of a 30-bedroom mansion where you've lived for 20 years. But also, of course, Sandringham is where the royal family, King and all, have traditionally spent Christmas,\n",
            "\n",
            "03:01â€“03:25 â€” and they're not going to want to be bumping into Prince Andrew at every step. Indeed. Now, in terms of next steps, there's some UK media reports this morning, which suggests that Andrew could be in line for a six-figure payout from the King to cover relocation costs. I mean, something like that, if it's true, is going to irk the British public who already largely believes\n",
            "\n",
            "03:25â€“03:50 â€” that he shouldn't be getting anything from the taxpayer now. No, I think the idea is he won't be getting anything from the taxpayer, not directly. This will come from King Charles' private wealth. I agree. On the one hand, there could easily be public thought of why should this man be subsidised. But on the other, from the palace's point of view,\n",
            "\n",
            "03:50â€“04:14 â€” they're not going to want to cast Andrew out completely into the cold. A, on a personal level, he is still their mother's son, but also on a practical one. If they do, he's going to be looking for sources of finance elsewhere. And heaven forbid the thought of where he might find them. And in terms of what you're hearing from the palace,\n",
            "\n",
            "04:14â€“04:19 â€” I mean, there's no kind of precedent for this in living memory, is there?\n",
            "\n",
            "04:20â€“04:44 â€” No, no, really there isn't. There, people have been removed from a place in the succession in the past, back in the days of World War I, when some of Queen Victoria's descendants were Germans and fighting on the other side. But something like this, no, it is genuinely unprecedented. Is there a risk with that?\n",
            "\n",
            "===============================================================\n",
            "Saved files:\n",
            "{'json': '/content/drive/MyDrive/transcripts/transcript_20251103_161701.json', 'srt': '/content/drive/MyDrive/transcripts/transcript_20251103_161701.srt', 'vtt': '/content/drive/MyDrive/transcripts/transcript_20251103_161701.vtt'}\n",
            "{'txt': '/content/drive/MyDrive/transcripts/transcript_20251103_161701.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¤ Upload any audio/video file (mp3, wav, mp4, mkv...) and transcribe\n",
        "from google.colab import files\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import os\n",
        "\n",
        "def _fmt_mmss(sec: float) -> str:\n",
        "    if sec < 0: sec = 0.0\n",
        "    m = int(sec // 60)\n",
        "    s = int(round(sec % 60))\n",
        "    return f\"{m:02d}:{s:02d}\"\n",
        "\n",
        "def _group_segments_into_paragraphs(\n",
        "    segments: List[Dict],\n",
        "    max_gap_s: float = 1.0,         # if silence gap > this, start new paragraph\n",
        "    max_chars: int = 350,           # soft cap per paragraph\n",
        "    max_duration_s: float = 25.0    # hard cap on paragraph length\n",
        ") -> List[Tuple[float, float, str]]:\n",
        "    \"\"\"\n",
        "    Returns a list of (start_sec, end_sec, paragraph_text).\n",
        "    \"\"\"\n",
        "    paras: List[Tuple[float, float, str]] = []\n",
        "    if not segments:\n",
        "        return paras\n",
        "\n",
        "    cur_start = float(segments[0][\"start\"])\n",
        "    cur_end   = float(segments[0][\"end\"])\n",
        "    cur_texts = [segments[0][\"text\"].strip()]\n",
        "    cur_chars = len(cur_texts[0])\n",
        "\n",
        "    prev_end = cur_end\n",
        "\n",
        "    for seg in segments[1:]:\n",
        "        s = float(seg[\"start\"])\n",
        "        e = float(seg[\"end\"])\n",
        "        t = seg[\"text\"].strip()\n",
        "\n",
        "        gap = max(0.0, s - prev_end)\n",
        "        next_chars = cur_chars + (1 if cur_texts else 0) + len(t)\n",
        "        next_duration = max(cur_end, e) - cur_start\n",
        "\n",
        "        # Decide if we should start a new paragraph\n",
        "        if (gap > max_gap_s) or (next_chars > max_chars) or (next_duration > max_duration_s):\n",
        "            # flush current paragraph\n",
        "            paras.append((cur_start, cur_end, \" \".join(cur_texts).strip()))\n",
        "            # start new\n",
        "            cur_start = s\n",
        "            cur_end   = e\n",
        "            cur_texts = [t]\n",
        "            cur_chars = len(t)\n",
        "        else:\n",
        "            # extend current\n",
        "            cur_end   = max(cur_end, e)\n",
        "            cur_texts.append(t)\n",
        "            cur_chars = next_chars\n",
        "\n",
        "        prev_end = e\n",
        "\n",
        "    # flush last\n",
        "    paras.append((cur_start, cur_end, \" \".join(cur_texts).strip()))\n",
        "    return paras\n",
        "\n",
        "def _save_paragraphs_txt(paragraphs: List[Tuple[float, float, str]], basepath_no_ext: str) -> str:\n",
        "    out_txt = basepath_no_ext + \".txt\"\n",
        "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "        for (st, en, txt) in paragraphs:\n",
        "            f.write(f\"{_fmt_mmss(st)}â€“{_fmt_mmss(en)} â€” {txt}\\n\\n\")\n",
        "    return out_txt\n",
        "\n",
        "uploaded = files.upload()  # Pick one file\n",
        "chosen = next(iter(uploaded)) if uploaded else None\n",
        "\n",
        "# Settings\n",
        "backend    = \"local\"      # \"local\" (offline) or \"openai\" (cloud)\n",
        "language   = None         # e.g., \"en\" / \"si\" / \"ta\" (optional)\n",
        "model_size = \"small\"      # tiny | base | small | medium | large-v3 (local only)\n",
        "\n",
        "if chosen:\n",
        "    out = transcribe_file_to_dir(chosen, SAVE_DIR, backend=backend, language=language, model_size=model_size)\n",
        "    paths = out[\"paths\"]\n",
        "    result = out[\"result\"]\n",
        "    segments = result.get(\"segments\", [])\n",
        "\n",
        "    # Build paragraphs\n",
        "    paragraphs = _group_segments_into_paragraphs(\n",
        "        segments,\n",
        "        max_gap_s=1.2,        # tweak if you want bigger/smaller paragraphs\n",
        "        max_chars=380,\n",
        "        max_duration_s=30.0\n",
        "    )\n",
        "\n",
        "    # Print full transcript as paragraphs with time ranges\n",
        "    print(\"========== FULL TRANSCRIPT (paragraphs with timestamps) ==========\\n\")\n",
        "    for (st, en, txt) in paragraphs:\n",
        "        print(f\"{_fmt_mmss(st)}â€“{_fmt_mmss(en)} â€” {txt}\\n\")\n",
        "\n",
        "    # Save a .txt version alongside JSON/SRT/VTT\n",
        "    base_no_ext = os.path.splitext(paths[\"json\"])[0]  # use same base name\n",
        "    txt_path = _save_paragraphs_txt(paragraphs, base_no_ext)\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"Saved files:\")\n",
        "    print(paths)                         # JSON / SRT / VTT\n",
        "    print({\"txt\": txt_path})             # Paragraphs TXT\n",
        "else:\n",
        "    print(\"No file selected.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "id": "VoVRk2V-MMDT",
        "outputId": "ccd2e15d-4ec9-4682-85a9-5b33ae31808e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ea5bb955-80f4-4c14-a1bc-31075377d986\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ea5bb955-80f4-4c14-a1bc-31075377d986\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving step_3.mp4 to step_3 (1).mp4\n",
            "========== FULL TRANSCRIPT (paragraphs with timestamps) ==========\n",
            "\n",
            "00:02â€“00:23 â€” Welcome back to the third and final episode of this series. In this video, we'll put everything we've built together so far and build our dashboard. Thank you so much for all the comments and feedback I have received. Let me know what you would like the next video to be about. And don't forget to subscribe to my channel to stay updated.\n",
            "\n",
            "00:23â€“00:41 â€” So let's do a quick recap on previous episodes. You have built your floor plan using SketchUp or Sweet Home 3D or any other 3D software. You have created overlays on top of your renders. And now let's build our dashboard.\n",
            "\n",
            "00:45â€“01:08 â€” Okay, never mind. You have not created any of these. That's okay. Just download the assets from the GitHub link in the description. Okay, so back to our series business. First, you need to install HA floor plan. I recommend downloading it from Hacks. We are also going to cover two configuration methods for dashboards. So let's begin.\n",
            "\n",
            "01:08â€“01:33 â€” Option A is where we control the dashboards from the user interface. Pretty straightforward and easy. So first, Home Assistant. Go to Settings, Dashboards. You will see two types of configured dashboards. Let's add a new dashboard from the user interface. Give it a title. Pick any icon you wish.\n",
            "\n",
            "01:33â€“02:01 â€” And then let's create it. And then let's open that called demo. Now go to the raw configuration editor. And then you need to paste the demo YAML file that you found in the GitHub repo. Awesome.\n",
            "\n",
            "02:01â€“02:29 â€” So you need to update the entities highlighted in red with your smart lamps entity name. And the highlighted elements in yellow must match the IDs we created in the SVG. And that's it. You have a fully functional digital twin. And let's try out the lamp. Works fine.\n",
            "\n",
            "02:29â€“02:57 â€” And we're done. Right, so option B, using YAML files to configure dashboards. It is a more advanced method, but I highly recommend it. And I use the following tools to make life easier. The first tool is Samba Share Add-on in Home Assistant, where you can mount Home Assistant storage on your computer.\n",
            "\n",
            "02:57â€“03:24 â€” And the add-on configuration is pretty straightforward, like adding username, password, allowing or enabling shares, in this case, conflict share, and also allowing the network cider block for your Home Assistant and your PC that lives on the same subnet. So far, so good. So now we have enabled Samba Share on Home Assistant add-ons.\n",
            "\n",
            "03:24â€“03:53 â€” Now we can mount it on our client computers. It can be a Mac, Linux, or Windows. And the second tool is VS Code and a handy VS Code extension called Home Assistant Config Helper. Now open VS Code and add the mounted drive into your workspace. Next, find your main configuration YAML file,\n",
            "\n",
            "03:53â€“04:19 â€” where we will add a dashboard section. Now let's copy this section and paste it into a new section and rename it to demo. Give it a new title. Let's call it demo YAML.\n",
            "\n",
            "04:19â€“04:45 â€” And also call it demo dashboard or demo YAML to differentiate from the UI one. So far, so good. So we are referencing a folder called, or directory called, dashboards and demo YAML file. So let's go to dashboards and then let's go to our downloaded assets,\n",
            "\n",
            "04:45â€“05:12 â€” get the demo YAML file, copy it and paste it under dashboards. We can rename it to YAML or YAML, same result really. Now let's talk about the Home Assistant Config Helper extension. We are going to change the language setting of the YAML file to use language Home Assistant. This will allow us to have autocomplete\n",
            "\n",
            "05:12â€“05:38 â€” on our actual device entity names configured in Home Assistant. Of course, that is after you have configured your extension correctly. Right, so let's save our work and go back to Home Assistant. Then let's go to the file editor and make sure that the changes we have done\n",
            "\n",
            "05:38â€“06:07 â€” did indeed reflect on the Home Assistant disk or drive. And we can see that indeed our demo YAML dashboard section is there, which is good. And next, we need to restart Home Assistant for these YAML-based dashboards. So let's restart. Then we will see a new dashboard appears on the left, demo YAML.\n",
            "\n",
            "06:07â€“06:32 â€” And if I try to edit this dashboard from the UI, it will fail, which is good. However, the dashboard is working fine, and it's exactly the same result as the UI dashboard. Great, so some final thoughts. I recommend arranging your rooms or views into separate files\n",
            "\n",
            "06:32â€“06:59 â€” and using the include statement to have this nested setup. This will help keeping things organized and easy to manage because these configuration files can get really, really long and messy and they become unmaintainable. So that's it. If you really like this video, please hit like and subscribe. Let me know what the next video series should be about.\n",
            "\n",
            "06:59â€“07:07 â€” Thank you so much and I enjoyed recording these videos. Have a great day. Bye.\n",
            "\n",
            "===============================================================\n",
            "Saved files:\n",
            "{'json': '/content/drive/MyDrive/transcripts/transcript_20251103_163645.json', 'srt': '/content/drive/MyDrive/transcripts/transcript_20251103_163645.srt', 'vtt': '/content/drive/MyDrive/transcripts/transcript_20251103_163645.vtt'}\n",
            "{'txt': '/content/drive/MyDrive/transcripts/transcript_20251103_163645.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **Transcribe a YouTube link**"
      ],
      "metadata": {
        "id": "OH2C6HQpMseh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Batch mode (many files/links at once)**"
      ],
      "metadata": {
        "id": "dm_yjUEXNedI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Process a list of local paths and/or YouTube URLs\n",
        "items = [\n",
        "    # \"/content/your_audio.mp3\",\n",
        "    # \"https://www.youtube.com/watch?v=xxxxxxxxxxx\",\n",
        "]\n",
        "\n",
        "backend    = \"local\"   # or \"openai\"\n",
        "language   = None\n",
        "model_size = \"small\"\n",
        "\n",
        "for item in items:\n",
        "    try:\n",
        "        if item.startswith(\"http\"):\n",
        "            out = transcribe_youtube_to_dir(item, SAVE_DIR, backend=backend, language=language, model_size=model_size)\n",
        "        else:\n",
        "            out = transcribe_file_to_dir(item, SAVE_DIR, backend=backend, language=language, model_size=model_size)\n",
        "        print(\"Saved:\", out[\"paths\"])\n",
        "    except Exception as e:\n",
        "        print(\"Failed:\", item, \"â†’\", e)\n"
      ],
      "metadata": {
        "id": "em1ITLTANgFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}